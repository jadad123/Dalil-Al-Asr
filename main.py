import requests
import feedparser
import json
import time
import base64
import sqlite3
import os
import re
import urllib.parse
import io
import urllib3
import random
import httpx
from openai import OpenAI
from datetime import datetime
from difflib import SequenceMatcher
from PIL import Image, ImageDraw, ImageFont

# ==========================================
# 0. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¹ØµØ±"
# ==========================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

OPENROUTER_KEY = os.getenv("OPENROUTER_KEY", "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")

WP_DOMAIN = os.getenv("WP_DOMAIN", "https://dalil-alasr.com") 
WP_USER = os.getenv("WP_USER", "admin")
WP_APP_PASS = os.getenv("WP_APP_PASS", "xxxx xxxx xxxx xxxx")

WP_ENDPOINT = f"{WP_DOMAIN}/wp-json/wp/v2"

# Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø°ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØ±
SITE_NAME_WATERMARK = "Dalil Al-Asr"

BROWSER_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8",
    "Referer": "https://google.com"
}

# Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©
FREE_TEXT_MODELS = [
    "google/gemini-2.0-flash-exp:free",
    "meta-llama/llama-3.3-70b-instruct:free",
    "qwen/qwen-2.5-72b-instruct:free", 
    "deepseek/deepseek-chat:free"
]

FREE_VISION_MODELS = [
    "google/gemini-2.0-flash-exp:free",
    "meta-llama/llama-3.2-90b-vision-instruct:free",
]

# ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ø¸ÙˆØ±Ø© (ÙÙ†ØŒ Ø£ØºØ§Ù†ÙŠ)
BLACKLIST_KEYWORDS = [
    "ÙÙŠÙ„Ù…", "Ù…Ø³Ù„Ø³Ù„", "Ø£ØºÙ†ÙŠØ©", "ÙÙ†Ø§Ù†", "Ù…Ù…Ø«Ù„Ø©", "Ø±Ù‚Øµ", "Ø­ÙÙ„ ØºÙ†Ø§Ø¦ÙŠ", 
    "Ø³ÙŠÙ†Ù…Ø§", "Ø¯Ø±Ø§Ù…Ø§", "Ø·Ø±Ø¨", "Ø£Ù„Ø¨ÙˆÙ…", "ÙƒÙ„ÙŠØ¨", "ÙØ¶Ø§Ø¦ÙŠØ§Øª", 
    "Movie", "Song", "Actress", "Cinema", "Music Video", "Concert"
]

DB_FILE = "/app/data/dalil_history.db" if os.path.exists("/app") else "dalil_history.db"

# ==========================================
# 1. Ø¯ÙˆØ§Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# ==========================================
def init_db():
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute('''CREATE TABLE IF NOT EXISTS history 
                 (link TEXT PRIMARY KEY, title TEXT, published_at TEXT)''')
    conn.commit()
    conn.close()

def is_published_link(link):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT 1 FROM history WHERE link=?", (link,))
    exists = c.fetchone()
    conn.close()
    return exists is not None

def mark_published(link, title):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("INSERT OR IGNORE INTO history VALUES (?, ?, ?)", (link, title, datetime.now().isoformat()))
    conn.commit()
    conn.close()

def is_duplicate_semantic(new_title):
    conn = sqlite3.connect(DB_FILE)
    c = conn.cursor()
    c.execute("SELECT title FROM history ORDER BY published_at DESC LIMIT 50")
    rows = c.fetchall()
    conn.close()
    if not rows: return False
    recent_titles = [row[0] for row in rows if row[0]]
    for existing in recent_titles:
        # Ù†Ø³Ø¨Ø© ØªØ·Ø§Ø¨Ù‚ Ø£Ù‚Ù„ (65%) Ù„Ø£Ù†Ù†Ø§ Ù†ØªØ±Ø¬Ù…ØŒ ÙØ§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† Ù‚Ø¯ ØªØ®ØªÙ„Ù Ù‚Ù„ÙŠÙ„Ø§Ù‹
        if SequenceMatcher(None, new_title.lower(), existing.lower()).ratio() > 0.65:
            return True
    return False

# ==========================================
# 2. Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ù‚Ø³Ø§Ù… (Auto Category Creator)
# ==========================================
def get_auth_header():
    clean_pass = WP_APP_PASS.replace(' ', '')
    creds = base64.b64encode(f"{WP_USER}:{clean_pass}".encode()).decode()
    return {"Authorization": f"Basic {creds}", "Content-Type": "application/json"}

def get_category_id_by_name(cat_name):
    """ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù‚Ø³Ù…ØŒ Ø¥Ø°Ø§ Ù„Ù… ÙŠØ¬Ø¯Ù‡ ÙŠÙ‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¦Ù‡"""
    # ØªØ±Ø¬Ù…Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¥Ù„Ù‰ Ø¹Ø±Ø¨ÙŠØ© Ù„Ø¥Ù†Ø´Ø§Ø¦Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹
    ARABIC_NAMES = {
        "News": "Ø£Ø®Ø¨Ø§Ø± Ø¹Ø§Ù…Ø©", "Politics": "Ø³ÙŠØ§Ø³Ø©", "Economy": "Ø§Ù‚ØªØµØ§Ø¯ ÙˆØ£Ø¹Ù…Ø§Ù„",
        "Crypto": "Ø¹Ù…Ù„Ø§Øª Ø±Ù‚Ù…ÙŠØ©", "Tech": "ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§ ÙˆØ°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ", 
        "Health": "ØµØ­Ø© ÙˆØ·Ø¨", "Science": "Ø¹Ù„ÙˆÙ… ÙˆÙØ¶Ø§Ø¡", "Tutorials": "Ø´Ø±ÙˆØ­Ø§Øª ÙˆØ£Ø¯Ù„Ø©",
        "Sports": "Ø±ÙŠØ§Ø¶Ø©"
    }
    
    final_name = ARABIC_NAMES.get(cat_name, cat_name)
    
    try:
        h = get_auth_header()
        # Ø§Ù„Ø¨Ø­Ø«
        r = requests.get(f"{WP_ENDPOINT}/categories?search={final_name}", headers=h)
        if r.status_code == 200 and r.json():
            return r.json()[0]['id']
        
        # Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯
        print(f"   ğŸ”¨ Creating Category: {final_name}...")
        r = requests.post(f"{WP_ENDPOINT}/categories", headers=h, json={"name": final_name})
        if r.status_code == 201:
            return r.json()['id']
    except: pass
    return 1 # Uncategorized fallback

def get_or_create_tag_id(tag_name):
    try:
        h = get_auth_header()
        r = requests.get(f"{WP_ENDPOINT}/tags?search={tag_name}", headers=h)
        if r.status_code == 200 and r.json(): return r.json()[0]['id']
        r = requests.post(f"{WP_ENDPOINT}/tags", headers=h, json={"name": tag_name})
        if r.status_code == 201: return r.json()['id']
    except: pass
    return None

# ==========================================
# 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø§Ø¦ÙŠØ©
# ==========================================
def get_ai_image_url(title):
    clean_title = re.sub(r'[^\w\s]', '', title)
    words = clean_title.split()[:9]
    prompt_text = " ".join(words)
    # Ø·Ù„Ø¨ ØµÙˆØ±Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¬Ø¯Ø§Ù‹
    final_prompt = f"Editorial news photo of {prompt_text}, highly detailed, realistic, 4k, journalism style, no text, no blur"
    encoded_prompt = urllib.parse.quote(final_prompt)
    seed = int(time.time())
    return f"https://image.pollinations.ai/prompt/{encoded_prompt}?width=1280&height=720&nologo=true&seed={seed}&model=flux"

def check_image_safety(image_url):
    print(f"   ğŸ” Checking watermark in original...")
    http_client = httpx.Client(verify=False, transport=httpx.HTTPTransport(local_address="0.0.0.0"))
    client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=OPENROUTER_KEY, http_client=http_client)
    
    for i in range(3):
        model = random.choice(FREE_VISION_MODELS)
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": [{"type": "text", "text": "Does this image contain ANY text, logos, or watermarks? Answer strictly 'YES' or 'NO'."}, {"type": "image_url", "image_url": {"url": image_url}}]}]
            )
            is_clean = "NO" in response.choices[0].message.content.strip().upper()
            return is_clean
        except: time.sleep(1)
    return False

def apply_branding(image_bytes):
    """ØªØ¶ÙŠÙ Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø§Ø¦ÙŠØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù…ÙˆÙ‚Ø¹Ùƒ Ø¹Ù„Ù‰ Ø£ÙŠ ØµÙˆØ±Ø©"""
    try:
        img = Image.open(io.BytesIO(image_bytes)).convert("RGBA")
        width, height = img.size
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø´ÙØ§ÙØ©
        overlay = Image.new("RGBA", img.size, (0, 0, 0, 0))
        draw = ImageDraw.Draw(overlay)
        
        # Ø´Ø±ÙŠØ· Ø£Ø³ÙˆØ¯ Ø´ÙØ§Ù ÙÙŠ Ø§Ù„Ø£Ø³ÙÙ„
        bar_height = int(height * 0.08) 
        draw.rectangle([(0, height - bar_height), (width, height)], fill=(0, 0, 0, 160))
        
        # ÙƒØªØ§Ø¨Ø© Ø§Ø³Ù… Ø§Ù„Ù…ÙˆÙ‚Ø¹
        try: 
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ø®Ø·ØŒ ÙˆØ¥Ø°Ø§ ÙØ´Ù„ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            font_size = int(bar_height * 0.6)
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", font_size)
            except:
                font = ImageFont.load_default()
        except: return image_bytes
        
        text = SITE_NAME_WATERMARK
        # Ø­Ø³Ø§Ø¨ Ù…ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ù„ÙŠÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ù†ØªØµÙ
        # (Ø¨Ù…Ø§ Ø£Ù† load_default Ù„Ø§ ÙŠØ¯Ø¹Ù… getbbox Ø¨Ø¯Ù‚Ø© ÙÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©ØŒ Ø³Ù†Ø¶Ø¹Ù‡ ØªÙ‚Ø±ÙŠØ¨ÙŠØ§Ù‹)
        text_x = width / 2 - (len(text) * 5) 
        text_y = height - (bar_height * 0.8)
        
        draw.text((text_x, text_y), text, font=font, fill=(255, 255, 255, 255))
        
        combined = Image.alpha_composite(img, overlay)
        output = io.BytesIO()
        combined.convert("RGB").save(output, format="JPEG", quality=92)
        return output.getvalue()
    except Exception as e:
        print(f"   âš ï¸ Branding Error: {e}")
        return image_bytes

def upload_final_image(img_url, alt_text):
    print(f"   â¬†ï¸ Downloading & Branding Image...")
    try:
        r_img = requests.get(img_url, headers=BROWSER_HEADERS, timeout=30, verify=False)
        if r_img.status_code == 200:
            # ğŸ’¡ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ù…Ù‡Ù…Ø©: ÙˆØ¶Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø§Ø¦ÙŠØ©
            branded_image_data = apply_branding(r_img.content)
            
            filename = f"dalil_{int(time.time())}.jpg"
            headers_wp = get_auth_header()
            headers_wp["Content-Disposition"] = f"attachment; filename={filename}"
            headers_wp["Content-Type"] = "image/jpeg"
            
            r_wp = requests.post(f"{WP_ENDPOINT}/media", headers=headers_wp, data=branded_image_data)
            if r_wp.status_code == 201: 
                media_id = r_wp.json()['id']
                seo_data = {
                    "alt_text": alt_text, 
                    "title": alt_text, 
                    "caption": f"Ø­Ù‚ÙˆÙ‚ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø­ÙÙˆØ¸Ø© Ù„Ù€ {SITE_NAME_WATERMARK}", 
                    "description": alt_text
                }
                requests.post(f"{WP_ENDPOINT}/media/{media_id}", headers=get_auth_header(), json=seo_data)
                return media_id
    except Exception as e: print(f"   âŒ Upload Error: {e}")
    return None

def extract_image(entry):
    if hasattr(entry, 'media_content') and entry.media_content:
        return entry.media_content[0].get('url') if isinstance(entry.media_content[0], dict) else entry.media_content[0]['url']
    if hasattr(entry, 'links') and entry.links:
        for l in entry.links:
            if 'image' in getattr(l, 'type', ''): return getattr(l, 'href', None)
    if hasattr(entry, 'summary'):
        m = re.search(r'<img.*?src=["\']([^"\']+)["\']', entry.summary)
        if m: return m.group(1)
    return None

# ==========================================
# 4. Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ù…Ø­Ø±Ø± ÙˆØµØ­ÙÙŠ)
# ==========================================
def generate_arabic_content(news_item):
    http_client = httpx.Client(verify=False, transport=httpx.HTTPTransport(local_address="0.0.0.0"))
    client = OpenAI(base_url="https://openrouter.ai/api/v1", api_key=OPENROUTER_KEY, http_client=http_client)
    
    prompt = f"""
    Ø£Ù†Øª Ø±Ø¦ÙŠØ³ ØªØ­Ø±ÙŠØ± Ù…ÙˆÙ‚Ø¹ "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¹ØµØ±". Ù…Ù‡Ù…ØªÙƒ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ (Ø¨Ø£ÙŠ Ù„ØºØ© ÙƒØ§Ù†Øª) ÙˆØ¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨ØªÙ‡ Ø¨Ù…Ù‚Ø§Ù„ Ø§Ø­ØªØ±Ø§ÙÙŠ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰.

    Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: "{news_item['title']}" - {news_item['summary']}

    Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„ØµØ§Ø±Ù…Ø©:
    1. **Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¶Ù…ÙˆÙ†:** Ù„Ø§ ØªØºÙŠØ± Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø£Ùˆ Ø§Ù„Ø£Ø±Ù‚Ø§Ù…. Ø§Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø± Ø¨Ø¯Ù‚Ø©.
    2. **Ø§Ù„Ù„ØºØ©:** Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰ØŒ Ù‚ÙˆÙŠØ©ØŒ ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù….
    3. **Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ© (HTML):**
       - Ø¹Ù†ÙˆØ§Ù† ÙØ±Ø¹ÙŠ <h2> Ù„Ù„Ù…Ù‚Ø¯Ù…Ø©.
       - ÙÙ‚Ø±Ø§Øª <p> (Direction: RTL).
       - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø®Ø¨Ø± Ø´Ø±Ø­Ø§Ù‹ (Tutorial)ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø®Ø·ÙˆØ§Øª Ù…Ø±Ù‚Ù…Ø© <ol><li>...</li></ol>.
       - ØµÙ†Ø¯ÙˆÙ‚ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª: <div style="background-color: #e3f2fd; padding: 15px; border-radius: 5px;"><strong>â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø³Ø±ÙŠØ¹Ø©:</strong> ...</div>
    4. **Ø§Ù„ØªØµÙ†ÙŠÙ:** ÙÙŠ Ø¢Ø®Ø± Ø³Ø·Ø±ØŒ Ø§Ø®ØªØ± Ù‚Ø³Ù…Ø§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:
       [Politics, Economy, Crypto, Tech, Science, Health, Sports, Tutorials, News]
       Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ ÙÙŠ Ø¢Ø®Ø± Ø³Ø·Ø±:
       CATEGORY: Tech
       TAGS: ÙƒÙ„Ù…Ø©1, ÙƒÙ„Ù…Ø©2, ÙƒÙ„Ù…Ø©3
       META_DESC: ÙˆØµÙ Ù‚ØµÙŠØ± ÙˆØ´ÙŠÙ‚ Ù„Ù„Ù…Ù‚Ø§Ù„ Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² 150 Ø­Ø±Ù.

    Ø§Ø¨Ø¯Ø£ Ø§Ù„ÙƒØªØ§Ø¨Ø© Ù…Ø¨Ø§Ø´Ø±Ø©.
    """
    
    for i in range(5):
        model = random.choice(FREE_TEXT_MODELS)
        try:
            print(f"   ğŸ¤– Writing with: {model}")
            response = client.chat.completions.create(
                model=model, messages=[{"role": "user", "content": prompt}], temperature=0.5
            )
            content = response.choices[0].message.content.replace("```html", "").replace("```", "").strip()
            return re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', content)
        except Exception as e:
            print(f"   âš ï¸ AI Error: {e}. Retrying...")
            time.sleep(2)
    return None

def publish_to_wp(title, content, feat_img_id):
    meta_desc, tags, cat_id = "", [], 1
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø±Ø¯ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
    try:
        if "CATEGORY:" in content:
            parts = content.split("CATEGORY:")
            body_content = parts[0].strip()
            metadata = parts[1]
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚Ø³Ù…
            cat_name = metadata.split("\n")[0].strip()
            cat_id = get_category_id_by_name(cat_name)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ³ÙˆÙ… ÙˆØ§Ù„ÙˆØµÙ
            if "TAGS:" in metadata:
                tags_part = metadata.split("TAGS:")[1].split("META_DESC:")[0]
                tags = [t.strip() for t in tags_part.split(",")]
            if "META_DESC:" in metadata:
                meta_desc = metadata.split("META_DESC:")[1].strip()
                
            content = body_content
    except: pass

    tag_ids = [get_or_create_tag_id(t) for t in tags if t]
    
    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ø£ÙŠ ÙƒÙ„Ù…Ø§Øª Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø²Ø§Ø¦Ø¯Ø© Ø¥Ø°Ø§ ÙˆØ¬Ø¯Øª
    clean_title = title # ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ù„ÙŠØªØ±Ø¬Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø£ÙŠØ¶Ø§Ù‹
    # Ù…Ù„Ø§Ø­Ø¸Ø©: ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¬Ø¹Ù„ Ø§Ù„Ù€ AI ÙŠØ¹Ø·ÙŠÙ†Ø§ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£ÙŠØ¶Ø§Ù‹.
    # Ø­Ø§Ù„ÙŠØ§Ù‹ Ø³Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø£Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ø±Ø¨ÙŠ ÙˆØ§Ù„Ø¹Ù†ÙˆØ§Ù† Ø£ØµÙ„ÙŠ (Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ)ØŒ
    # Ø³Ø£Ø¶ÙŠÙ ØªØ¹Ø¯ÙŠÙ„Ø§Ù‹ Ø¨Ø³ÙŠØ·Ø§Ù‹ Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø¹Ø±Ø¨ÙŠØ§Ù‹ Ø£ÙŠØ¶Ø§Ù‹ ÙÙŠ Ø§Ù„Ù€ Prompt Ø§Ù„Ù‚Ø§Ø¯Ù…ØŒ
    # Ù„ÙƒÙ† Ø­Ø§Ù„ÙŠØ§Ù‹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‡Ùˆ Ø§Ù„Ø£Ù‡Ù….
    
    data = {
        "title": title, # Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ø£ØµÙ„ÙŠØŒ ÙŠÙØ¶Ù„ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ù„ÙŠØ¹Ø·ÙŠ Ø¹Ù†ÙˆØ§Ù†Ø§Ù‹ Ø¹Ø±Ø¨ÙŠØ§Ù‹
        "content": content, "status": "publish",
        "categories": [cat_id], "tags": tag_ids, "excerpt": meta_desc,
        "featured_media": feat_img_id,
        "rank_math_focus_keyword": tags[0] if tags else "Ø£Ø®Ø¨Ø§Ø±",
        "rank_math_description": meta_desc
    }
    data["meta"] = {"rank_math_focus_keyword": data["rank_math_focus_keyword"], "rank_math_description": meta_desc}
    
    r = requests.post(f"{WP_ENDPOINT}/posts", headers=get_auth_header(), json=data)
    if r.status_code == 201: return r.json()['link']
    print(f"   âŒ Publish Failed: {r.status_code}")
    return None

# ==========================================
# 5. Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªÙ†ÙˆØ¹Ø© (ÙƒØ«Ø§ÙØ© Ø¹Ø§Ù„ÙŠØ©)
# ==========================================
def main():
    print("ğŸš€ Dalil Al-Asr (Content Machine V2) Started...")
    init_db()
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø¶Ø®Ù…Ø© Ù…Ù† Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙ†ÙˆØ¹
    feeds = [
        # --- ÙƒØ±ÙŠØ¨ØªÙˆ ÙˆÙ…Ø§Ù„ ---
        "https://cointelegraph.com/rss",
        "https://decrypt.co/feed",
        "https://www.coindesk.com/arc/outboundfeeds/rss/",
        "https://sa.investing.com/rss/news.rss",
        "https://www.cnbcarabia.com/rss/latest-news",
        
        # --- ØªÙ‚Ù†ÙŠØ© ÙˆØ°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ---
        "https://techcrunch.com/feed/",
        "https://www.theverge.com/rss/index.xml",
        "https://aitnews.com/feed/",
        "https://wired.com/feed/rss",
        "https://www.unlimit-tech.com/feed/",
        
        # --- Ø³ÙŠØ§Ø³Ø© ÙˆØ¹Ø§Ù„Ù… ---
        "https://www.aljazeera.net/aljazeerarss/a7c186be-1baa-4bd4-9d80-a84db769f779/73d0e1b4-532f-45ef-b135-bfdff8b8cab9",
        "https://www.skynewsarabia.com/web/rss",
        "https://cnn.com/rss/cnn_topstories.rss",
        "https://feeds.bbci.co.uk/news/world/rss.xml",
        
        # --- Ø¹Ù„ÙˆÙ… ÙˆØµØ­Ø© ---
        "https://www.sciencedaily.com/rss/top/health.xml",
        "https://www.nasa.gov/rss/dyn/breaking_news.rss"
    ]
    
    while True:
        print(f"\nâ° Cycle Start: {datetime.now().strftime('%H:%M')}")
        random.shuffle(feeds) # Ø®Ù„Ø· Ø§Ù„Ù…ØµØ§Ø¯Ø± Ù„Ø¹Ø¯Ù… Ù†Ø´Ø± Ù†ÙØ³ Ø§Ù„Ù†ÙˆØ¹ Ù…ØªØªØ§Ù„ÙŠØ§Ù‹
        
        for feed in feeds:
            try:
                d = feedparser.parse(feed)
                # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ Ø¥Ù„Ù‰ 5 Ù„Ù…Ù„Ø¡ Ø§Ù„Ù…ÙˆÙ‚Ø¹
                for entry in d.entries[:5]: 
                    if is_published_link(entry.link): continue
                    
                    # ÙÙ„ØªØ± Ø§Ù„ÙÙ†
                    if any(bad in entry.title for bad in BLACKLIST_KEYWORDS):
                        continue
                        
                    if is_duplicate_semantic(entry.title): continue
                    
                    print(f"   âš¡ Processing: {entry.title[:50]}...")
                    
                    # 1. Ø§Ù„ØµÙˆØ±Ø©
                    original_img = extract_image(entry)
                    final_img_url, img_source = None, ""
                    
                    if original_img:
                        # Ù†ÙØ­Øµ Ù‡Ù„ Ù‡ÙŠ Ù†Ø¸ÙŠÙØ©ØŸ
                        if check_image_safety(original_img):
                            print("   âœ… Original Image Clean. Branding it...")
                            final_img_url = original_img
                        else:
                            print("   âš ï¸ Original has watermark. Generating AI Image...")
                            final_img_url = get_ai_image_url(entry.title)
                            img_source = "AI"
                    else:
                        print("   ğŸ¨ No image found. Generating AI Image...")
                        final_img_url = get_ai_image_url(entry.title)
                        img_source = "AI"
                    
                    # Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ù…Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù…Ø© Ø§Ù„Ù…Ø§Ø¦ÙŠØ©
                    fid = upload_final_image(final_img_url, entry.title)
                    
                    if fid:
                        # 2. Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
                        content = generate_arabic_content({'title': entry.title, 'summary': getattr(entry, 'summary', '')})
                        if content:
                            link = publish_to_wp(entry.title, content, fid)
                            if link:
                                print(f"   âœ… Published: {link}")
                                mark_published(entry.link, entry.title)
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ù‚ØµÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª (Ø³Ø±Ø¹Ø© Ø£Ø¹Ù„Ù‰)
                    time.sleep(10) 
            except Exception as e: 
                print(f"   âš ï¸ Feed Error: {str(e)[:50]}")
        
        print("ğŸ’¤ Short Rest (10 min)...")
        time.sleep(600)

if __name__ == "__main__":
    main()
